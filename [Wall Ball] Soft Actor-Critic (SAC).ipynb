{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+PLWt7o3fH4JHXJiUDT81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-mujoco-tennis/blob/main/%5BWall%20Ball%5D%20Soft%20Actor-Critic%20(SAC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wall Ball: Soft Actor-Critic (SAC)"
      ],
      "metadata": {
        "id": "Z5m28j-y778h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mujoco\n",
        "\n",
        "# Set up GPU rendering.\n",
        "from google.colab import files\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "# Check if installation was succesful.\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Other imports and helper functions\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n"
      ],
      "metadata": {
        "id": "CRcmb1y7XABJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "r5g-qyU5l1Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "import mujoco\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "import numpy\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import platform\n",
        "from importlib.metadata import version\n",
        "import matplotlib\n",
        "import matplotlib.pyplot\n",
        "from gymnasium import utils\n",
        "from gymnasium.envs.mujoco import MujocoEnv\n",
        "from gymnasium.spaces import Box"
      ],
      "metadata": {
        "id": "WY4o6uzNlqb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Mujoco Version: {version('mujoco')}\")\n",
        "print(f\"Stable-Baselines3 Version: {version('stable-baselines3')}\")\n",
        "print(f\"Matplotlib Version: {version('matplotlib')}\")"
      ],
      "metadata": {
        "id": "W93vyNlZmDW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rl_type = \"SAC\"\n",
        "env_str = \"WallBall\"\n",
        "log_dir = \"./logs/{}\".format(env_str)\n",
        "name_prefix = \"wall_ball\""
      ],
      "metadata": {
        "id": "_pWMBaSjbSyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = {\n",
        "    \"env_str\": env_str,\n",
        "    \"rl_type\": rl_type,\n",
        "    \"eval_freq\": 25_000,\n",
        "    \"n_envs\": 4,\n",
        "    \"min_force\": 100.0,\n",
        "    \"total_timesteps\": 2_000_000,\n",
        "    \"log_dir\": log_dir\n",
        "}"
      ],
      "metadata": {
        "id": "EBQMuLuKPAcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoRecordCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_path: str,\n",
        "        video_length: int,\n",
        "        save_freq: int = 5_000,\n",
        "        name_prefix: str =\"rl_model\",\n",
        "        verbose: int = 0):\n",
        "\n",
        "        super().__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.video_length = video_length\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseAlgorithm\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env # type: VecEnv\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # num_timesteps = n_envs * n times env.step() was called\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = {}  # type: Dict[str, Any]\n",
        "        # self.globals = {}  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
        "        # Sometimes, for event callback, it is useful\n",
        "        # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.save_freq == 0:\n",
        "\n",
        "          name_prefix = f\"{self.name_prefix}_{self.num_timesteps}\"\n",
        "\n",
        "          # Record video of the best model playing\n",
        "          rec_val = make_vec_env(make_env, n_envs=1)\n",
        "          rec_val = VecVideoRecorder(rec_val,\n",
        "                                    self.save_path,\n",
        "                                    video_length=self.video_length,\n",
        "                                    record_video_trigger=lambda x: x == 0,\n",
        "                                    name_prefix=name_prefix)\n",
        "\n",
        "          obs = rec_val.reset()\n",
        "          session_length = 0\n",
        "          total_reward = 0.0\n",
        "          csv_file_name = os.path.join(self.save_path, f\"{name_prefix}.csv\")\n",
        "          with open(csv_file_name, 'w') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile, delimiter=',')\n",
        "            csv_writer.writerow([\"Sensor Data\",\n",
        "                                 \"Reward\",\n",
        "                                 \"Total Reward\",\n",
        "                                 \"Done\"])\n",
        "            for _ in range(self.video_length):\n",
        "              session_length += 1\n",
        "              action, _states = self.model.predict(obs)\n",
        "              obs, rewards, dones, info = rec_val.step(action)\n",
        "              total_reward += rewards\n",
        "              #print(info)\n",
        "              csv_writer.writerow([info[0][\"sensor_data\"],\n",
        "                                   rewards[0],\n",
        "                                   total_reward[0],\n",
        "                                   dones[0]])\n",
        "              rec_val.render()\n",
        "\n",
        "              if dones:\n",
        "                break\n",
        "\n",
        "          print(f\"Step: {self.num_timesteps} | Session Length: {session_length} |Total Bounces: {int(total_reward[0])}\")\n",
        "\n",
        "          rec_val.close()\n",
        "        return True"
      ],
      "metadata": {
        "id": "D8uMGsZgOWfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zl8FYadlNQq"
      },
      "outputs": [],
      "source": [
        "class WallBallEnv(MujocoEnv, utils.EzPickle):\n",
        "    metadata = {\n",
        "        \"render_modes\": [\n",
        "            \"human\",\n",
        "            \"rgb_array\",\n",
        "            \"depth_array\",\n",
        "        ],\n",
        "        \"render_fps\": 100,\n",
        "    }\n",
        "\n",
        "    # set default episode_len for truncate episodes\n",
        "    def __init__(self, episode_len=750, min_force=0.0, **kwargs):\n",
        "        utils.EzPickle.__init__(self, **kwargs)\n",
        "\n",
        "        self.min_force = min_force\n",
        "        self.bounce_count = 0\n",
        "        self.previous_touch_value = 0\n",
        "        # change shape of observation to your observation space size\n",
        "        observation_space = Box(low=-numpy.inf, high=np.inf, shape=(18,), dtype=numpy.float64)\n",
        "        # load your MJCF model with env and choose frames count between actions\n",
        "        MujocoEnv.__init__(\n",
        "            self,\n",
        "            os.path.abspath(f\"/content/{name_prefix}.xml\"),\n",
        "            5,\n",
        "            observation_space=observation_space,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.step_number = 0\n",
        "        self.episode_len = episode_len\n",
        "\n",
        "    # determine the reward depending on observation or other properties of the simulation\n",
        "    def step(self, a):\n",
        "        reward = 0.0\n",
        "        self.do_simulation(a, self.frame_skip)\n",
        "        self.step_number += 1\n",
        "\n",
        "\n",
        "        # Bounce Detection (CRITICAL)\n",
        "        current_touch_value = self.data.sensor(\"touch_sensor\").data[0]\n",
        "        if current_touch_value >= self.min_force and self.previous_touch_value <= 0:  # Check for transition from no contact to contact\n",
        "            self.bounce_count += 1\n",
        "            reward = 1.0\n",
        "            #print(f\"Bounce! Total Bounces: {self.bounce_count}\")\n",
        "        self.previous_touch_value = current_touch_value\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        done = bool(not np.isfinite(obs).all() or (obs[2] < 0))\n",
        "        truncated = self.step_number > self.episode_len\n",
        "        return obs, reward, done, truncated, {\"sensor_data\": current_touch_value}\n",
        "\n",
        "    # define what should happen when the model is reset (at the beginning of each episode)\n",
        "    def reset_model(self):\n",
        "        self.step_number = 0\n",
        "        self.bounce_count = 0\n",
        "\n",
        "        # for example, noise is added to positions and velocities\n",
        "        qpos = self.init_qpos + self.np_random.uniform(\n",
        "            size=self.model.nq, low=-0.01, high=0.01\n",
        "        )\n",
        "        qvel = self.init_qvel + self.np_random.uniform(\n",
        "            size=self.model.nv, low=-0.01, high=0.01\n",
        "        )\n",
        "        self.set_state(qpos, qvel)\n",
        "        return self._get_obs()\n",
        "\n",
        "    # determine what should be added to the observation\n",
        "    # for example, the velocities and positions of various joints can be obtained through their names, as stated here\n",
        "    def _get_obs(self):\n",
        "        obs = np.concatenate((np.array(self.data.joint(\"ball_freejoint\").qpos[:3]),\n",
        "                              np.array(self.data.joint(\"ball_freejoint\").qvel[:3]),\n",
        "                              np.array(self.data.joint(\"rotate_x\").qpos),\n",
        "                              np.array(self.data.joint(\"rotate_x\").qvel),\n",
        "                              np.array(self.data.joint(\"rotate_y\").qpos),\n",
        "                              np.array(self.data.joint(\"rotate_y\").qvel),\n",
        "                              np.array(self.data.joint(\"rotate_z\").qpos),\n",
        "                              np.array(self.data.joint(\"rotate_z\").qvel),\n",
        "                              np.array(self.data.joint(\"slider_x\").qpos),\n",
        "                              np.array(self.data.joint(\"slider_x\").qvel),\n",
        "                              np.array(self.data.joint(\"slider_y\").qpos),\n",
        "                              np.array(self.data.joint(\"slider_y\").qvel),\n",
        "                              np.array(self.data.joint(\"slider_z\").qpos),\n",
        "                              np.array(self.data.joint(\"slider_z\").qvel)), axis=0)\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure environment XML (MuJoCo model) is available\n",
        "xml_content = \"\"\"\n",
        "<mujoco model=\"tennis_wall_paddle\">\n",
        "    <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\" />\n",
        "    <option integrator=\"RK4\" timestep=\"0.002\" />\n",
        "    <size nconmax=\"500\" njmax=\"1000\" nstack=\"300000\" />\n",
        "    <default>\n",
        "        <joint armature=\"0.1\" damping=\"1\" limited=\"true\" />\n",
        "        <geom condim=\"3\" density=\"1000\" friction=\"1 0.5 0.5\" margin=\"0.001\" rgba=\"0.8 0.8 0.8 1\" />\n",
        "        <motor ctrlrange=\"-1 1\" ctrllimited=\"true\" />\n",
        "    </default>\n",
        "    <asset>\n",
        "        <texture builtin=\"flat\" height=\"1278\" name=\"texplane\" rgb1=\"0.9 0.9 0.9\" rgb2=\"0.1 0.1 0.1\" type=\"2d\" width=\"1279\" />\n",
        "        <material name=\"matplane\" reflectance=\"0.5\" shininess=\"0.1\" specular=\"0.1\" texture=\"texplane\" />\n",
        "    </asset>\n",
        "    <worldbody>\n",
        "        <light diffuse=\".8 .8 .8\" pos=\"0 0 3\" specular=\"0.1 0.1 0.1\" />\n",
        "        <geom name=\"floor\" pos=\"0 0 0\" size=\"5 5 0.1\" type=\"plane\" material=\"matplane\" />\n",
        "        <geom name=\"wall\" pos=\"3 0 1.5\" size=\"0.1 5 1.5\" type=\"box\" rgba=\"0.7 0.7 0.9 1\" />\n",
        "        <body name=\"paddle_base\" pos=\"-2 0 1\">\n",
        "            <joint name=\"paddle_slide_y\" type=\"slide\" axis=\"0 1 0\" range=\"-2 2\" />\n",
        "            <joint name=\"paddle_slide_z\" type=\"slide\" axis=\"0 0 1\" range=\"0.5 2\" />\n",
        "            <geom name=\"paddle_base_geom\" pos=\"0 0 0\" size=\"0.1 0.1 0.1\" type=\"box\"\n",
        "                rgba=\"0.2 0.2 0.8 1\" />\n",
        "\n",
        "            <body name=\"paddle_handle\" pos=\"0 0 0.15\">\n",
        "                <joint name=\"paddle_rotate_x\" type=\"hinge\" axis=\"1 0 0\" range=\"-90 90\" />\n",
        "                <geom name=\"paddle_handle_geom\" pos=\"0 0 0\" size=\"0.05 0.05 0.15\" type=\"box\" rgba=\"0.2 0.2 0.8 1\" />\n",
        "                <body name=\"paddle_head\" pos=\"0 0 0.15\">\n",
        "                    <geom name=\"paddle_head_geom\" pos=\"0 0 0.05\" size=\"0.2 0.05 0.2\" type=\"box\" rgba=\"0.8 0.2 0.2 1\" />\n",
        "                </body>\n",
        "            </body>\n",
        "        </body>\n",
        "        <body name=\"ball\" pos=\"-4 0 1.5\">\n",
        "            <joint name=\"ball_x\" type=\"free\" limited=\"false\" />\n",
        "            <geom name=\"ball_geom\" pos=\"0 0 0\" size=\"0.1\" type=\"sphere\" rgba=\"1 0 0 1\" />\n",
        "        </body>\n",
        "    </worldbody>\n",
        "    <actuator>\n",
        "        <motor name=\"paddle_slide_y\" joint=\"paddle_slide_y\" gear=\"100\" />\n",
        "        <motor name=\"paddle_slide_z\" joint=\"paddle_slide_z\" gear=\"100\" />\n",
        "        <motor name=\"paddle_rotate_x\" joint=\"paddle_rotate_x\" gear=\"100\" />\n",
        "    </actuator>\n",
        "</mujoco>\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{name_prefix}.xml\", \"w\") as f:\n",
        "    f.write(xml_content)"
      ],
      "metadata": {
        "id": "WSabiKwgmS-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = BallBounceEnv(render_mode=\"rgb_array\")\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Wks5iHMc_zz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env():\n",
        "  env = WallBallEnv(render_mode=\"rgb_array\",\n",
        "                      min_force=hyperparams[\"min_force\"])\n",
        "  check_env(env)\n",
        "  return env\n",
        "\n",
        "# Create Training environment\n",
        "env = make_vec_env(make_env,\n",
        "                   n_envs=hyperparams[\"n_envs\"],\n",
        "                   monitor_dir=os.path.join(log_dir, \"monitor\"))\n",
        "\n",
        "# Create Evaluation environment\n",
        "env_val = make_vec_env(make_env, n_envs=1)\n",
        "\n",
        "eval_callback = EvalCallback(env_val,\n",
        "                             best_model_save_path=log_dir,\n",
        "                             log_path=log_dir,\n",
        "                             render=False,\n",
        "                             deterministic=True,\n",
        "                             n_eval_episodes=20,\n",
        "                             eval_freq=hyperparams[\"eval_freq\"])\n",
        "\n",
        "video_record_callback = VideoRecordCallback(\n",
        "    save_path=os.path.join(log_dir, \"videos\"),\n",
        "    video_length=10_000,\n",
        "    save_freq=hyperparams[\"eval_freq\"],\n",
        "    name_prefix=name_prefix)\n",
        "\n",
        "# Create the callback list\n",
        "callbackList = CallbackList([video_record_callback,\n",
        "                             eval_callback])\n",
        "\n",
        "# learning with tensorboard logging and saving model\n",
        "model = SAC(\"MlpPolicy\",\n",
        "            env,\n",
        "            verbose=0,\n",
        "            tensorboard_log=os.path.join(log_dir, \"tensorboard\"))\n",
        "\n",
        "model.learn(total_timesteps=hyperparams[\"total_timesteps\"],\n",
        "            callback=callbackList,\n",
        "            progress_bar=False)\n",
        "\n",
        "# Save the model\n",
        "model.save(os.path.join(log_dir, \"final_model\"))\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env)\n",
        "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "env.close()\n",
        "env_val.close()"
      ],
      "metadata": {
        "id": "31kWylPTqRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Evaluation environment\n",
        "env_val = make_vec_env(make_env, n_envs=1)\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = os.path.join(log_dir, \"best_model\")\n",
        "best_model = SAC.load(best_model_path, env=env)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(best_model,\n",
        "                                          env_val,\n",
        "                                          n_eval_episodes=20)\n",
        "\n",
        "print(f\"Best Model - Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# Record video of the best model\n",
        "env = VecVideoRecorder(env_val, os.path.join(log_dir, \"videos\"),\n",
        "                       video_length=10_000,\n",
        "                       record_video_trigger=lambda x: x == 0,\n",
        "                       name_prefix=\"best_model_{}\".format(name_prefix))\n",
        "\n",
        "total_reward = 0\n",
        "obs = env.reset()\n",
        "for _ in range(10_000):\n",
        "    action, _states = best_model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    total_reward += rewards\n",
        "    env.render()\n",
        "    if dones:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "print(f\"Total reward: {total_reward[0]}\")"
      ],
      "metadata": {
        "id": "TiwmYjHCasmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the evaluations.npz file\n",
        "data = numpy.load(os.path.join(log_dir, \"evaluations.npz\"))\n",
        "\n",
        "# Extract the relevant data\n",
        "timesteps = data['timesteps']\n",
        "results = data['results']\n",
        "\n",
        "# Calculate the mean and standard deviation of the results\n",
        "mean_results = numpy.mean(results, axis=1)\n",
        "std_results = numpy.std(results, axis=1)\n",
        "\n",
        "# Plot the results\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(timesteps, mean_results)\n",
        "matplotlib.pyplot.fill_between(timesteps,\n",
        "                               mean_results - std_results,\n",
        "                               mean_results + std_results,\n",
        "                               alpha=0.3)\n",
        "\n",
        "matplotlib.pyplot.xlabel('Timesteps')\n",
        "matplotlib.pyplot.ylabel('Mean Reward')\n",
        "matplotlib.pyplot.title(f\"{rl_type} Performance on {env_str}\")\n",
        "matplotlib.pyplot.show()"
      ],
      "metadata": {
        "id": "eWL1-laWawXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62HctLv03Ewz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}